{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1\n",
      "\t[5, 5], 64 /2\n",
      "conv2/1\n",
      "\t[1, 1], 64 /1\n",
      "\t[3, 3], 64 /1\n",
      "\t[1, 1], 256 /1\n",
      "conv2/2\n",
      "\t[1, 1], 64 /1\n",
      "\t[3, 3], 64 /1\n",
      "\t[1, 1], 256 /1\n",
      "conv2/3\n",
      "\t[1, 1], 64 /1\n",
      "\t[3, 3], 64 /1\n",
      "\t[1, 1], 256 /1\n",
      "conv3/1\n",
      "\t[1, 1], 128 /2\n",
      "\t[3, 3], 128 /1\n",
      "\t[1, 1], 512 /1\n",
      "conv3/2\n",
      "\t[1, 1], 128 /1\n",
      "\t[3, 3], 128 /1\n",
      "\t[1, 1], 512 /1\n",
      "conv3/3\n",
      "\t[1, 1], 128 /1\n",
      "\t[3, 3], 128 /1\n",
      "\t[1, 1], 512 /1\n",
      "conv3/4\n",
      "\t[1, 1], 128 /1\n",
      "\t[3, 3], 128 /1\n",
      "\t[1, 1], 512 /1\n",
      "conv4/1\n",
      "\t[1, 1], 256 /2\n",
      "\t[3, 3], 256 /1\n",
      "\t[1, 1], 1024 /1\n",
      "conv4/2\n",
      "\t[1, 1], 256 /1\n",
      "\t[3, 3], 256 /1\n",
      "\t[1, 1], 1024 /1\n",
      "conv4/3\n",
      "\t[1, 1], 256 /1\n",
      "\t[3, 3], 256 /1\n",
      "\t[1, 1], 1024 /1\n",
      "conv4/4\n",
      "\t[1, 1], 256 /1\n",
      "\t[3, 3], 256 /1\n",
      "\t[1, 1], 1024 /1\n",
      "conv4/5\n",
      "\t[1, 1], 256 /1\n",
      "\t[3, 3], 256 /1\n",
      "\t[1, 1], 1024 /1\n",
      "conv4/6\n",
      "\t[1, 1], 256 /1\n",
      "\t[3, 3], 256 /1\n",
      "\t[1, 1], 1024 /1\n",
      "conv5/1\n",
      "\t[1, 1], 512 /2\n",
      "\t[3, 3], 512 /1\n",
      "\t[1, 1], 2048 /1\n",
      "conv5/2\n",
      "\t[1, 1], 512 /1\n",
      "\t[3, 3], 512 /1\n",
      "\t[1, 1], 2048 /1\n",
      "conv5/3\n",
      "\t[1, 1], 512 /1\n",
      "\t[3, 3], 512 /1\n",
      "\t[1, 1], 2048 /1\n",
      "fc1\n",
      "\t[4096, 1024]\n",
      "logits\n",
      "\t[1024, 5]\n",
      "Training KaggleDR_ResNet50 ...\n",
      "Iter 0/2000000  Avg.Batch Loss: 1.905506  Current Batch Loss: 1.905506  Learn.rate : 0.005  balanced\n",
      "Iter 0/2000000, Validation once in 20000 steps...\n",
      "Evaluating KaggleDR_ResNet50 ...\n",
      "Accuracy : 0.74546\n",
      "Onset level = 1\t ROC-AUC: 0.50000\n",
      "Onset level = 2\t ROC-AUC: 0.50000\n",
      "Current best : 0\t New best : 0.5\n",
      "A better model found. Saving the model in path: ./modelstore/KaggleDR_ResNet50regConst_0.0001lr_0.005BatchReNorm_original.ckpt\n"
     ]
    }
   ],
   "source": [
    "from models.ResNet50 import ResNet50\n",
    "from utils.Reader import AdvancedReader\n",
    "from utils.MyTimer import MyTimer\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "onset = -1\n",
    "train_source = '/gpfs01/berens/user/mayhan/kaggle_dr_data/train_JF_BG_512/'\n",
    "test_source = '/gpfs01/berens/user/mayhan/kaggle_dr_data/test_JF_BG_512/'\n",
    "\n",
    "network_config = dict([('instance_shape', [512, 512, 3]),\n",
    "                       ('num_classes', 5),\n",
    "                       ('conv_depths', [3, 4, 6, 3]),\n",
    "                       ('num_filters', [[64, 64, 256], [128, 128, 512], [256, 256, 1024], [512, 512, 2048]]),\n",
    "                       ('fc_depths', [1024]),\n",
    "                       ('_lambda', 0.0001),\n",
    "                       ('lr', 0.005),\n",
    "                       ('decay_steps', 50000),\n",
    "                       ('decay_rate', 0.9),\n",
    "                       ('data_aug', True), \n",
    "                       ('batch_renorm', True),\n",
    "                       ('preactivation', False),\n",
    "                       ('max_iter', 2000000),\n",
    "                       ('oversampling_limit', 0.2),\n",
    "                       ('batch_size', 8), # ResNet50: Max batch sizes allowed by BatchNorm and BatchReNorm are 14 and 8, respectively.\n",
    "                       ('val_step', 20000),\n",
    "                       ('quick_dirty_val', False)\n",
    "                      ])\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    drTrain = AdvancedReader(source=train_source, file_type = '.jpeg', \n",
    "                             csv_file='/gpfs01/berens/user/mayhan/kaggle_dr_data/trainLabels.csv', \n",
    "                             onset_level = onset, mode = 'train'\n",
    "                            )\n",
    "    model = ResNet50(instance_shape = network_config['instance_shape'], \n",
    "                     num_classes = network_config['num_classes'], \n",
    "                     name='KaggleDR_ResNet50'\n",
    "                    )\n",
    "    model.build(conv_stack_depths = network_config['conv_depths'],\n",
    "                num_filters = network_config['num_filters'], \n",
    "                fc_depths = network_config['fc_depths'],\n",
    "                _lambda = network_config['_lambda'], \n",
    "                learning_rate = network_config['lr'], \n",
    "                decay_steps = network_config['decay_steps'], \n",
    "                decay_rate = network_config['decay_rate'],\n",
    "                data_aug = network_config['data_aug'], \n",
    "                use_batch_renorm = network_config['batch_renorm'], \n",
    "                preactivation = network_config['preactivation']\n",
    "               )\n",
    "    model.initialize()\n",
    "    \n",
    "    with MyTimer('bazinga'):\n",
    "        model.train(tr_reader=drTrain, max_iter=network_config['max_iter'], \n",
    "                    batch_size=network_config['batch_size'], normalize=True, \n",
    "                    oversampling_threshold=network_config['oversampling_limit'],\n",
    "                    val_step=network_config['val_step'], quick_dirty_val=network_config['quick_dirty_val'],\n",
    "                    val_source=test_source\n",
    "                   )\n",
    "    \n",
    "    model.finalize()\n",
    "    \n",
    "# X-entropy across iterations\n",
    "plt.figure()\n",
    "plt.plot(range(0, len(model.diagnostics['losses'])), model.diagnostics['losses'], \n",
    "         color='b', linestyle='-', label='Minibatch loss')\n",
    "plt.plot(range(0, len(model.diagnostics['avg_losses'])), model.diagnostics['avg_losses'], \n",
    "         color='c', linestyle='-.', label='Avg. loss', linewidth=2)\n",
    "plt.xlabel(\"iteration\")\n",
    "plt.ylabel(\"Avg. cross entropy\")\n",
    "plt.title(\"ResNet50\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Validation performance across iterations\n",
    "plt.figure()\n",
    "x = np.multiply(network_config['val_step'], list(range(0, len(model.diagnostics['val_roc1']))))\n",
    "y = model.diagnostics['val_roc1']\n",
    "plt.plot(x, y, color='r', linestyle='-', label='onset 1')\n",
    "\n",
    "x = np.multiply(network_config['val_step'], list(range(0, len(model.diagnostics['val_roc2']))))\n",
    "y = model.diagnostics['val_roc2']\n",
    "plt.plot(x, y, color='g', linestyle='-.', label='onset 2')\n",
    "plt.xlabel(\"iteration\")\n",
    "plt.ylabel(\"ROC-AUC on validation set\")\n",
    "plt.title(\"ResNet50\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "diagnostics2save = model.diagnostics\n",
    "\n",
    "# Now, save the diagnostic results\n",
    "RESULTS_DIR = './RESULTS/'\n",
    "\n",
    "key = 'ResNet50_'\n",
    "if not network_config['batch_renorm']:\n",
    "    key = key + 'BatchNorm_'\n",
    "else:\n",
    "    key = key + 'BatchReNorm_'\n",
    "\n",
    "if network_config['preactivation']:\n",
    "    key = key + 'Preactivation'\n",
    "else:\n",
    "    key = key + 'Original'\n",
    "\n",
    "result_file_name = RESULTS_DIR + key + '_DIAG.pkl'\n",
    "with open(result_file_name, 'wb') as filehandler:\n",
    "    pickle.dump(diagnostics2save, filehandler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.ResNet50 import ResNet50\n",
    "from utils.Reader import AdvancedReader\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import pickle\n",
    "\n",
    "tf.reset_default_graph()\n",
    "with tf.Session() as sess:\n",
    "    model = ResNet50(instance_shape = network_config['instance_shape'], \n",
    "                     num_classes = network_config['num_classes'], \n",
    "                     name='KaggleDR_ResNet50'\n",
    "                    )\n",
    "    model.build(conv_stack_depths = network_config['conv_depths'],\n",
    "                num_filters = network_config['num_filters'], \n",
    "                fc_depths = network_config['fc_depths'],\n",
    "                _lambda = network_config['_lambda'], \n",
    "                learning_rate = network_config['lr'], \n",
    "                decay_steps = network_config['decay_steps'], \n",
    "                decay_rate = network_config['decay_rate'],\n",
    "                data_aug = network_config['data_aug'], \n",
    "                use_batch_renorm = network_config['batch_renorm'], \n",
    "                preactivation = network_config['preactivation']\n",
    "               )\n",
    "    \n",
    "    # No init. or training. Just restore the variables for model.\n",
    "    model.saver.restore(sess, model.model_path)\n",
    "    model.session = sess\n",
    "    \n",
    "    print('=======================================================\\nEvaluating the performance on TRAINING set')\n",
    "    dr = AdvancedReader(source=train_source,\n",
    "                        csv_file='/gpfs01/berens/user/mayhan/kaggle_dr_data/trainLabels.csv', \n",
    "                        mode = 'valtest' # valtest to read all from the training set\n",
    "                       )\n",
    "    labels_1hot_tr, predictions_1hot_tr, roc_auc_tr_onset1, roc_auc_tr_onset2 = model.inference(dr, \n",
    "                                                                                                batch_size=network_config['batch_size'], \n",
    "                                                                                                quick_dirty=network_config['quick_dirty_val'])\n",
    "    \n",
    "    print('=======================================================\\nEvaluating the performance on VALIDATION set')\n",
    "    dr = AdvancedReader(source=test_source,\n",
    "                        csv_file='/gpfs01/berens/user/mayhan/kaggle_dr_data/retinopathy_solution.csv', \n",
    "                        mode = 'val'\n",
    "                       )\n",
    "    labels_1hot_val, predictions_1hot_val, roc_auc_val_onset1, roc_auc_val_onset2 = model.inference(dr, \n",
    "                                                                                                    batch_size=network_config['batch_size'], \n",
    "                                                                                                    quick_dirty=network_config['quick_dirty_val'])\n",
    "\n",
    "    print('=======================================================\\nEvaluating the performance on TEST set')\n",
    "    dr = AdvancedReader(source=test_source,\n",
    "                        csv_file='/gpfs01/berens/user/mayhan/kaggle_dr_data/retinopathy_solution.csv', \n",
    "                        mode = 'test'\n",
    "                       )\n",
    "    labels_1hot_te, predictions_1hot_te, roc_auc_te_onset1, roc_auc_te_onset2 = model.inference(dr, \n",
    "                                                                                                batch_size=network_config['batch_size'],\n",
    "                                                                                                quick_dirty=network_config['quick_dirty_val'])\n",
    "    \n",
    "    print('=======================================================\\nEvaluating the performance on VAL and TEST sets combined')\n",
    "    dr = AdvancedReader(source=test_source,\n",
    "                        csv_file='/gpfs01/berens/user/mayhan/kaggle_dr_data/retinopathy_solution.csv', \n",
    "                        mode = 'valtest'\n",
    "                       )\n",
    "    labels_1hot_valte, predictions_1hot_valte, roc_auc_valte_onset1, roc_auc_valte_onset2 = model.inference(dr, \n",
    "                                                                                                            batch_size=network_config['batch_size'],\n",
    "                                                                                                            quick_dirty=network_config['quick_dirty_val'])\n",
    "        \n",
    "#### Now, save the results\n",
    "result = {}\n",
    "result['train_labels_1hot'] = labels_1hot_tr\n",
    "result['val_labels_1hot'] = labels_1hot_val\n",
    "result['test_labels_1hot'] = labels_1hot_te\n",
    "result['valtest_labels_1hot'] = labels_1hot_valte\n",
    "result['train_pred_1hot'] = predictions_1hot_tr\n",
    "result['val_pred_1hot'] = predictions_1hot_val\n",
    "result['test_pred_1hot'] = predictions_1hot_te\n",
    "result['valtest_pred_1hot'] = predictions_1hot_valte\n",
    "\n",
    "result_file_name = RESULTS_DIR + key + '.pkl'\n",
    "with open(result_file_name, 'wb') as filehandler:\n",
    "    pickle.dump(result, filehandler)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "### Test-time data augmentation for predictive uncertainty estimation\n",
    "##################################################\n",
    "\n",
    "from models.ResNet50 import ResNet50\n",
    "from utils.Reader import AdvancedReader\n",
    "from utils.DataAugmentation import data_augmentation\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import pickle\n",
    "\n",
    "\n",
    "onset = -1\n",
    "train_source = '/gpfs01/berens/user/mayhan/kaggle_dr_data/train_JF_BG_512/'\n",
    "test_source = '/gpfs01/berens/user/mayhan/kaggle_dr_data/test_JF_BG_512/'\n",
    "\n",
    "\n",
    "def feed_dict(model, x_batch, y_batch, _iter=1, max_iter=1):\n",
    "    progress = float(_iter) / float(max_iter)\n",
    "    if progress < 0.05:  # up to this point, use BatchNorm alone\n",
    "        rmax = 1.\n",
    "        rmin = 1.\n",
    "        dmax = 0.\n",
    "    else:  # then, gradually increase the clipping values\n",
    "        rmax = np.exp(2. * progress)  # 1.5\n",
    "        rmin = 1. / rmax\n",
    "        dmax = np.exp(2.5 * progress) - 1  # 2.\n",
    "    if progress > 0.95:\n",
    "        rmin = 0.\n",
    "    \n",
    "    feed_dict = {model.inputs: x_batch,\n",
    "                 model.labels: y_batch,\n",
    "                 model.is_training: False,\n",
    "                 model.rmin: [rmin],\n",
    "                 model.rmax: [rmax],\n",
    "                 model.dmax: [dmax]\n",
    "                }\n",
    "    return feed_dict\n",
    "\n",
    "def inference_with_test_time_data_aug(reader, model, test_input, test_input_aug, T=32, k=-1):\n",
    "    labels_all = []\n",
    "    predictions_all = []\n",
    "    \n",
    "    kkk = 0\n",
    "    \n",
    "    while not reader.exhausted_test_cases:\n",
    "        org_ex, label, _ = reader.next_batch(batch_size=1, normalize=True, shuffle=False)\n",
    "        \n",
    "        feed_img = {test_input: org_ex}\n",
    "        images = []\n",
    "        labels = []\n",
    "        for i in range(T):\n",
    "            aug_ex = np.squeeze(model.session.run([test_input_aug], feed_dict=feed_img))\n",
    "            images.append(aug_ex)\n",
    "            labels.append(label)\n",
    "            \n",
    "        x_batch = np.reshape(np.asarray(images, dtype=np.float32), [-1, 512, 512, 3])\n",
    "        y_batch = np.reshape(np.asarray(labels, dtype=np.float32), [-1, 1])\n",
    "        \n",
    "        predictions, labels = model.session.run([model.predictions_1hot, model.labels_1hot], \n",
    "                                                feed_dict=feed_dict(model, x_batch, y_batch)\n",
    "                                               )        \n",
    "        labels_all.append(labels[0])\n",
    "        predictions_all.append(predictions)        \n",
    "        \n",
    "        if k != -1:\n",
    "            k = k - 1\n",
    "            if k == 0:\n",
    "                dr.exhausted_test_cases = True\n",
    "            print('k = %d' % k)\n",
    "        kkk = kkk + 1\n",
    "        if kkk % 1000 == 0:\n",
    "            print('kkk = %d' % kkk)\n",
    "    print('kkk = %d' % kkk)\n",
    "    \n",
    "    # Convert from a list of M items of size Tx5 to an array of dims MxTx5. For labels_1hot: Mx5.   \n",
    "    labels_1hot = np.asarray(labels_all)\n",
    "    \n",
    "    predictions_all = np.asarray(predictions_all)\n",
    "    \n",
    "    # use the median of T predictions for the final class membership: Mx1x5\n",
    "    predictions_1hot_median = np.median(predictions_all, axis=1)\n",
    "    \n",
    "    correct = np.equal(np.argmax(labels_1hot, axis=1), np.argmax(predictions_1hot_median, axis=1))\n",
    "    acc = np.mean(np.asarray(correct, dtype=np.float32))\n",
    "    print('Accuracy : %.5f' % acc)\n",
    "        \n",
    "    onset_level = 1\n",
    "    labels_bin = np.greater_equal(np.argmax(labels_1hot, axis=1), onset_level)\n",
    "    pred_bin = np.sum(predictions_all[:, :, onset_level:], axis=2) # MxTx1\n",
    "    pred_bin_median = np.median(pred_bin, axis=1) # Mx1x1  \n",
    "    fpr, tpr, _ = roc_curve(labels_bin, np.squeeze(pred_bin_median))\n",
    "    roc_auc_onset1 = auc(fpr, tpr)\n",
    "    print('Onset level = %d\\t ROC-AUC: %.5f' % (onset_level, roc_auc_onset1))\n",
    "            \n",
    "    onset_level = 2\n",
    "    labels_bin = np.greater_equal(np.argmax(labels_1hot, axis=1), onset_level)\n",
    "    pred_bin = np.sum(predictions_all[:, :, onset_level:], axis=2) # MxTx1\n",
    "    pred_bin_median = np.median(pred_bin, axis=1) # Mx1x1  \n",
    "    fpr, tpr, _ = roc_curve(labels_bin, np.squeeze(pred_bin_median))\n",
    "    roc_auc_onset1 = auc(fpr, tpr)\n",
    "    print('Onset level = %d\\t ROC-AUC: %.5f' % (onset_level, roc_auc_onset1))\n",
    "        \n",
    "    return labels_1hot, predictions_all\n",
    "    \n",
    "    \n",
    "# Now, reset the graph and create a new session in which to run the model\n",
    "tf.reset_default_graph()\n",
    "with tf.Session() as sess:\n",
    "    model = ResNet50(instance_shape = network_config['instance_shape'], \n",
    "                     num_classes = network_config['num_classes'], \n",
    "                     name='KaggleDR_ResNet50'\n",
    "                    )\n",
    "    model.build(conv_stack_depths = network_config['conv_depths'],\n",
    "                num_filters = network_config['num_filters'], \n",
    "                fc_depths = network_config['fc_depths'],\n",
    "                _lambda = network_config['_lambda'], \n",
    "                learning_rate = network_config['lr'], \n",
    "                decay_steps = network_config['decay_steps'], \n",
    "                decay_rate = network_config['decay_rate'],\n",
    "                data_aug = network_config['data_aug'], \n",
    "                use_batch_renorm = network_config['batch_renorm'], \n",
    "                preactivation = network_config['preactivation']\n",
    "               )\n",
    "    \n",
    "    # No init.  or training. Just restore the variables for model.\n",
    "    model.saver.restore(sess, model.model_path)\n",
    "    model.session = sess\n",
    "    \n",
    "    # Now, set up the data augmentation components of the graph to be used during test time.\n",
    "    # These components do not belong to the model.     \n",
    "    test_input = tf.placeholder(dtype=tf.float32, shape=[None, 512, 512, 3], name='test_input')\n",
    "    test_input_aug = data_augmentation(test_input)\n",
    "    # end of test-time augmentation subnetwork\n",
    "    \n",
    "    T = 4 # number of MC samples\n",
    "    k = 10  # early stop threshold for inference (for a quick evaluation)\n",
    "    \n",
    "    print('=======================================================\\nEvaluating the performance on TRAINING set')\n",
    "    dr = AdvancedReader(source=train_source,\n",
    "                        csv_file='/gpfs01/berens/user/mayhan/kaggle_dr_data/trainLabels.csv', \n",
    "                        mode = 'valtest' # valtest to read all from the training set\n",
    "                       )\n",
    "    labels_1hot_tr, predictions_1hot_tr_ttaug = inference_with_test_time_data_aug(reader=dr, model=model, \n",
    "                                                                                  test_input=test_input, \n",
    "                                                                                  test_input_aug=test_input_aug, \n",
    "                                                                                  T=T, k=k)\n",
    "    \n",
    "    print('=======================================================\\nEvaluating the performance on VALIDATION set')\n",
    "    dr = AdvancedReader(source=test_source,\n",
    "                        csv_file='/gpfs01/berens/user/mayhan/kaggle_dr_data/retinopathy_solution.csv', \n",
    "                        mode = 'val'\n",
    "                       )\n",
    "    labels_1hot_val, predictions_1hot_val_ttaug = inference_with_test_time_data_aug(reader=dr, model=model, \n",
    "                                                                                    test_input=test_input, \n",
    "                                                                                    test_input_aug=test_input_aug, \n",
    "                                                                                    T=T, k=k)\n",
    "    \n",
    "    print('=======================================================\\nEvaluating the performance on TEST set') \n",
    "    dr = AdvancedReader(source=test_source,\n",
    "                        csv_file='/gpfs01/berens/user/mayhan/kaggle_dr_data/retinopathy_solution.csv', \n",
    "                        mode = 'test'\n",
    "                       )\n",
    "    labels_1hot_te, predictions_1hot_te_ttaug = inference_with_test_time_data_aug(reader=dr, model=model, \n",
    "                                                                                  test_input=test_input, \n",
    "                                                                                  test_input_aug=test_input_aug, \n",
    "                                                                                  T=T, k=k)\n",
    "    \n",
    "    print('=======================================================\\nEvaluating the performance on VAL and TEST sets combined')\n",
    "    dr = AdvancedReader(source=test_source,\n",
    "                        csv_file='/gpfs01/berens/user/mayhan/kaggle_dr_data/retinopathy_solution.csv', \n",
    "                        mode = 'valtest'\n",
    "                       )\n",
    "    labels_1hot_valte, predictions_1hot_valte_ttaug = inference_with_test_time_data_aug(reader=dr, model=model, \n",
    "                                                                                        test_input=test_input, \n",
    "                                                                                        test_input_aug=test_input_aug, \n",
    "                                                                                        T=T, k=k)\n",
    "  \n",
    "    \n",
    "#### Now, save the results\n",
    "result_ttaug = {}\n",
    "result_ttaug['train_labels_1hot'] = labels_1hot_tr\n",
    "result_ttaug['val_labels_1hot'] = labels_1hot_val\n",
    "result_ttaug['test_labels_1hot'] = labels_1hot_te\n",
    "result_ttaug['valtest_labels_1hot'] = labels_1hot_valte\n",
    "result_ttaug['train_pred_1hot'] = predictions_1hot_tr_ttaug\n",
    "result_ttaug['val_pred_1hot'] = predictions_1hot_val_ttaug\n",
    "result_ttaug['test_pred_1hot'] = predictions_1hot_te_ttaug\n",
    "result_ttaug['valtest_pred_1hot'] = predictions_1hot_valte_ttaug\n",
    "\n",
    "result_file_name = RESULTS_DIR + key + '_TTAUG.pkl'\n",
    "with open(result_file_name, 'wb') as filehandler:\n",
    "    pickle.dump(result_ttaug, filehandler)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
